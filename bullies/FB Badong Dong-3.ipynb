{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238778af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "314f8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75b546a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_IDs=list()\n",
    "\n",
    "with open('/Users/levtsipes/Desktop/Work/DeMAS/Bully/fb .txt','r') as file:    \n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        #print(re.findall(r\"https\\:\\/\\/www.facebook.com\\/([^;]*)\\n\",line))\n",
    "        group_IDs+=re.findall(r\"https\\:\\/\\/www.facebook.com\\/([^;]*)\\n\",line)\n",
    "group_IDs=group_IDs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "996403ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-96-79ca214c6e3a>:26: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Edge(r'/Users/levtsipes/Downloads/edgedriver_mac64/msedgedriver',\n",
      "<ipython-input-96-79ca214c6e3a>:76: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  div_class_post = driver.find_elements_by_tag_name('div')\n",
      "<ipython-input-96-79ca214c6e3a>:112: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  div_class_comment = driver.find_elements_by_tag_name('div')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-96-79ca214c6e3a>:159: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  div_class_member = driver.find_elements_by_tag_name('div')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Final Project\n",
    "#Introduction to Programming with Python and R\n",
    "#Author: Budong Dong\n",
    "\n",
    "#Objective:\n",
    "#This script is used to scrap the users' ID number and their URL in Facebook groups.\n",
    "#Since Facebook no longer provides API for private groups, this script uses the Selenium package to simulate web browsing activities using Edge browser.\n",
    "\n",
    "#In order to accomplish this, necessary libraries are downloaded and imported into the current Python session.\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "\n",
    "#Later we will write the crawled data into a csv file, so library `pandas` is downloaded and imported.\n",
    "import pandas as pd\n",
    "#We will also add the date/time into the filename of the written file, so `time` is downloaded and imported.\n",
    "import time\n",
    "\n",
    "#First, we need to start an Microsoft Edge webdriver.\n",
    "edge_options = webdriver.EdgeOptions()\n",
    "edge_options.add_argument('--disable-notifications')\n",
    "\n",
    "#The webdriver is downloaded to local and loaded.\n",
    "driver = webdriver.Edge(r'/Users/levtsipes/Downloads/edgedriver_mac64/msedgedriver',\n",
    "                        options=edge_options)  # USER INPUT - file location\n",
    "#Getting the website that we want to visit - here we use Facebook\n",
    "driver.get('https://www.facebook.com/')\n",
    "\n",
    "#Locating the input areas for login information, email and password\n",
    "email = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name = 'email']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name = 'pass']\")))\n",
    "\n",
    "#Clearing any existing content in the email and password area\n",
    "email.clear()\n",
    "password.clear()\n",
    "\n",
    "#Using `send_keys` method to enter your Facebook login email and password into the areas above\n",
    "email.send_keys(\"r.roku@yandex.ru\")  # USER INPUT\n",
    "password.send_keys(\"QuintoDeiStampi__2019\")  # USER INPUT\n",
    "\n",
    "#Login by submitting the email and password and pressing the `submit` button\n",
    "login = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type = 'submit']\"))).click()\n",
    "\n",
    "#Maximizing the browser window to crawl as many lines of data as possible\n",
    "driver.maximize_window()# maximize window\n",
    "\n",
    "#Setting the number of scrolls.\n",
    "n_scrolls = 15  # Each scroll has approximately 10 new users. Use (group size/10 + safety margin). Don't try this on a huge group, FB could time out\n",
    "\n",
    "#Specifying the Name and group ID that you want to crawl data from.\n",
    "#Here we are interested in a group of depression patients.\n",
    "\n",
    "#group_names = ['Российский Союз ветеранов', 'Ветераны Афганистана ОКСВА','ВЕТЕРАНЫ АФГАНА','ТА Типичная Анорексичка']\n",
    "\n",
    "for i in range(len(group_names)):\n",
    "    try:\n",
    "        #group_name = group_names[i]  # insert group name here\n",
    "        group_ID = group_IDs[i]  # insert group id here\n",
    "\n",
    "        #Getting to the specified facebook group. The URL is concatenated by coercing 'https://www.facebook.com/groups/ and the user-input group ID\n",
    "        driver.get('https://www.facebook.com/' + group_ID)  # USER INPUT # Insert group number\n",
    "\n",
    "\n",
    "        #Simulating the mouse scrolls to get load as much data as possible. There is a 5-second break between each scroll to avoid FB's robot detection.\n",
    "        for i in range(0, n_scrolls):\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "            time.sleep(5)\n",
    "\n",
    "        #Extracting the URL and ID of all post senders\n",
    "        #First, creat an empty list to collect users info who make posts into the interested group\n",
    "        users_post = list()\n",
    "        #Using the `find_elements_by_tag_name` method to find all html lines with tag `div`\n",
    "        div_class_post = driver.find_elements_by_tag_name('div')\n",
    "        #Looking through each div tag for our interested information\n",
    "        for div in div_class_post:\n",
    "            #By inspecting the HTML, the post maker information is stored under div class == \"q676j6op qypqp5cg\"\n",
    "            if div.get_attribute('class') == \"q676j6op qypqp5cg\":\n",
    "                #Extracting all anchor tags under this div class\n",
    "                anchor_post =div.find_elements_by_tag_name('a')\n",
    "                #Looking through each anchor tag for the user URL (in attribute `href`) and username (in attribute `aria-label`)\n",
    "                for a in anchor_post:\n",
    "                    #Extracting `a href` and a area-label`\n",
    "                    a_href=a.get_attribute('href')\n",
    "                    a_aria_label = a.get_attribute('aria-label')\n",
    "                    #Filtering only the relevant URLs which has a fixed format of 'https://www.facebook.com/groups/' + group_ID + '/user'.\n",
    "                    if a_href is not None: #and a_aria_label is not None and str(a_href).startswith('https://www.facebook.com/groups/' + group_ID + '/user'):\n",
    "                        #Creating a dictionary of username-url pairs.\n",
    "                        url_post = a_href\n",
    "                        username_post = a_aria_label\n",
    "                        user_post = [username_post, url_post]\n",
    "                        #Appending the username-url pairs to the users_post list\n",
    "                        users_post.append(user_post)\n",
    "\n",
    "        #Writing the list containing username-url pairs to a csv file and indicating the content, group, written time in the file name.\n",
    "        \n",
    "          \n",
    "        x = r'/Users/levtsipes/Desktop/Work/DeMAS/Bully/nf/POST MAKERS_result_' + re.findall(r'([^/]+)',group_ID)[0] + \"_\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()) + '.csv'\n",
    "        filepath = Path(x)  \n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)        \n",
    "        df = pd.DataFrame(users_post, columns=[\"username\",\"url\"])\n",
    "        df.to_csv(filepath)\n",
    "\n",
    "\n",
    "\n",
    "        #Extract the URL and ID of all comment makers\n",
    "        #First, creat an empty list to collect users info who make comments into the interested group\n",
    "        comment_makers = list()\n",
    "        #Using the `find_elements_by_tag_name` method to find all html lines with tag `div`\n",
    "        div_class_comment = driver.find_elements_by_tag_name('div')\n",
    "        #Looking through each div tag for our interested information\n",
    "        for div in div_class_comment:\n",
    "            #By inspecting the HTML, the comment information is stored under div class == \"tw6a2znq sj5x9vvc d1544ag0 cxgpxx05\"\n",
    "            if div.get_attribute('class') == \"tw6a2znq sj5x9vvc d1544ag0 cxgpxx05\":\n",
    "                #Extracting all span tags under this div class\n",
    "                span_class_comment = div.find_elements_by_tag_name('span')\n",
    "                #Looking through each span tag\n",
    "                for span in span_class_comment:\n",
    "                    # By inspecting the HTML, the comment maker information is stored under span class == \"nc684nl6\".\n",
    "                    if span.get_attribute('class') == \"nc684nl6\":\n",
    "                        #Extracting all anchor tags under this span class\n",
    "                        anchor_comment = span.find_elements_by_tag_name('a')\n",
    "                        #Looking through all anchor tags for url (in a href)\n",
    "                        for a in anchor_comment:\n",
    "                            #Extracting all urls under this tag\n",
    "                            a_href = a.get_attribute('href')\n",
    "                            #Filtering only the relevant urls with fixed format 'https://www.facebook.com/groups/' + group_ID + '/user'\n",
    "                            if a_href is not None:\n",
    "                                #Creating a dictionary of userID-url pairs\n",
    "                                url_comment = a_href\n",
    "                                user_ID=re.findall(r\"https\\:\\/\\/www.facebook.com\\/([^;]*)\\?\",url_comment) #UserID is extracted from URL using regular expression\n",
    "                                comment_maker = [user_ID, url_comment]\n",
    "                                #Sppending the userID-URL pairs to the comment_makers list\n",
    "                                comment_makers.append(comment_maker)\n",
    "\n",
    "        #Writing the list containing userID-URL pairs to a csv file and indicating the content, group, written time in the file name.\n",
    "        y = r'/Users/levtsipes/Desktop/Work/DeMAS/Bully/nf/COMMENT_MAKERS_result_' + re.findall(r'([^/]+)',group_ID)[0] + \"_\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()) + '.csv'\n",
    "        filepath = Path(y)  \n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)        \n",
    "        df = pd.DataFrame(comment_makers, columns=[\"user_ID\",\"url\"])\n",
    "        df.to_csv(filepath)\n",
    "\n",
    "        try:\n",
    "            driver.get('https://www.facebook.com/' + group_ID + 'members')  # USER INPUT # Insert group number\n",
    "            print('yes!')\n",
    "            members = list()\n",
    "\n",
    "\n",
    "            #Simulating the mouse scrolls to get load as much data as possible. There is a 5-second break between each scroll to avoid FB's robot detection.\n",
    "            for j in range(0, n_scrolls):\n",
    "                driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "                time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "            #Using the `find_elements_by_tag_name` method to find all html lines with tag `div`\n",
    "            div_class_member = driver.find_elements_by_tag_name('div')\n",
    "            #Looking through each div tag for our interested information\n",
    "            for div in div_class_member:\n",
    "                #By inspecting the HTML, the comment information is stored under div class == \"tw6a2znq sj5x9vvc d1544ag0 cxgpxx05\"\n",
    "                if div.get_attribute('class') == \"qzhwtbm6 knvmm38d\":\n",
    "                    #Extracting all span tags under this div class\n",
    "                    span_class_member = div.find_elements_by_tag_name('span')\n",
    "                    #print(span_class_member)\n",
    "                    #Looking through each span tag\n",
    "                    for span in span_class_member:\n",
    "                        # By inspecting the HTML, the comment maker information is stored under span class == \"nc684nl6\".\n",
    "                        if span.get_attribute('class') == \"nc684nl6\":\n",
    "                            #Extracting all anchor tags under this span class\n",
    "                            anchor_member = span.find_elements_by_tag_name('a')\n",
    "                            #print(anchor_member)\n",
    "                            #Looking through all anchor tags for url (in a href)\n",
    "                            for a in anchor_member:\n",
    "                                #Extracting all urls under this tag\n",
    "                                a_href = a.get_attribute('href')\n",
    "                                #print(a_href)\n",
    "                                #Filtering only the relevant urls with fixed format 'https://www.facebook.com/groups/' + group_ID + '/user'\n",
    "                                if a_href is not None:\n",
    "                                    #Creating a dictionary of userID-url pairs\n",
    "                                    url_member = a_href\n",
    "                                    user_ID=re.findall(r'https\\:\\/\\/www.facebook.com\\/'+group_ID+r\"user\\/(.*)\\/\",url_member) #UserID is extracted from URL using regular expression\n",
    "                                    actual_member = [user_ID, url_member]\n",
    "                                    #Sppending the userID-URL pairs to the comment_makers list\n",
    "                                    members.append(actual_member)\n",
    "                                    \n",
    "            if len(members)!=0:\n",
    "\n",
    "                z = r'/Users/levtsipes/Desktop/Work/DeMAS/Bully/nf/MEMBERS_result_' + re.findall(r'([^/]+)',group_ID)[0] + \"_\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()) + '.csv'\n",
    "                filepath = Path(z)  \n",
    "                filepath.parent.mkdir(parents=True, exist_ok=True)        \n",
    "                df = pd.DataFrame(comment_makers, columns=[\"user_ID\",\"url\"])\n",
    "                df.to_csv(filepath)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        print('exception:', group_ID)\n",
    "    \n",
    "    \n",
    "    os.system('say \"group is finished\"')\n",
    "\n",
    "os.system('say \"Code is finished\"')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
