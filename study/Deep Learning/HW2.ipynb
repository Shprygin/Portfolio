{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7jnJBN8k6x1"
      },
      "source": [
        "##  Backprop training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOFJEdDvk6x2"
      },
      "source": [
        "In this notebook I do the backprop excercise manually to comaper performance of with- and without bias neural networks with manual and AutoGrad calucluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I set stable weights and training set for each model so that the results would be comparable"
      ],
      "metadata": {
        "id": "Qg6HccLwiyCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "input_size = 3\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(batch_size, input_size)\n",
        "y = torch.randn(batch_size, output_size)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(input_size, hidden_size)\n",
        "w2 = torch.randn(hidden_size, output_size)\n",
        "\n",
        "# Randomly initialize bias\n",
        "b1 = torch.randn(hidden_size)\n",
        "b2 = torch.randn(output_size) \n",
        "\n",
        "w1_2 = torch.clone(w1)\n",
        "w2_2 = torch.clone(w2)\n",
        "b1_2 = torch.clone(b1)\n",
        "b2_2 = torch.clone(b2)\n",
        "\n",
        "w1_3 = torch.clone(w1)\n",
        "w2_3 = torch.clone(w2)\n",
        "b1_3 = torch.clone(b1)\n",
        "b2_3 = torch.clone(b2)\n",
        "w1_3.requires_grad=True\n",
        "w2_3.requires_grad=True\n",
        "b1_3.requires_grad=True\n",
        "b2_3.requires_grad=True\n",
        "\n",
        "w1_4 = torch.clone(w1)\n",
        "w2_4 = torch.clone(w2)\n",
        "b1_4 = torch.clone(b1)\n",
        "b2_4 = torch.clone(b2)\n",
        "w1_4.requires_grad=True\n",
        "w2_4.requires_grad=True\n",
        "b1_4.requires_grad=True\n",
        "b2_4.requires_grad=True\n"
      ],
      "metadata": {
        "id": "ePDjk5CEfIPk"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, a model without bias"
      ],
      "metadata": {
        "id": "b1XMED09i06y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    #TODO\n",
        "    h_1 = x.mm(w1)\n",
        "    h_relu = h_1.clamp(min=0)\n",
        "    out = h_relu.mm(w2)\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = (out - y).pow(2).sum().item()\n",
        "    \n",
        "    # Backward pass: \n",
        "    dloss_dout = 2 * (out - y)\n",
        "    \n",
        "    grad_w2 = h_relu.t().mm(dloss_dout) \n",
        "    \n",
        "    grad_h_relu = dloss_dout.mm(w2.t())\n",
        "    \n",
        "    grad_h_relu[h_1 < 0] = 0\n",
        "    \n",
        "    grad_w1 = x.t().mm(grad_h_relu)\n",
        "    \n",
        "    \n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "    if t % 100 == 99:\n",
        "        print('Loss on iteration {} = {}'.format(t, loss))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLXRbfElGQ34",
        "outputId": "4f59b320-286a-4281-9420-6dc51ef9cb3a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on iteration 99 = 65.89350891113281\n",
            "Loss on iteration 199 = 65.64342498779297\n",
            "Loss on iteration 299 = 65.40166473388672\n",
            "Loss on iteration 399 = 65.16791534423828\n",
            "Loss on iteration 499 = 64.94190216064453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias included:"
      ],
      "metadata": {
        "id": "aSZkYs7Ti36t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdwl9Pr5k6x2",
        "outputId": "0dd47711-7cc5-4a67-a987-ac9ebf4f75c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on iteration 99 = 68.4677734375\n",
            "Loss on iteration 199 = 68.021728515625\n",
            "Loss on iteration 299 = 67.59562683105469\n",
            "Loss on iteration 399 = 67.18849182128906\n",
            "Loss on iteration 499 = 66.79944610595703\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    #TODO\n",
        "    h_1 = x.mm(w1_2)\n",
        "    h_1_b = h_1.add(b1_2)\n",
        "    h_relu = h_1_b.clamp(min=0)\n",
        "    h_2 = h_relu.mm(w2_2)\n",
        "    out = h_2.add(b2_2)\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = (out - y).pow(2).sum().item()\n",
        "    \n",
        "    # Backward pass: \n",
        "    dloss_dout = 2 * (out - y)\n",
        "    \n",
        "    dout_dh2 =  1\n",
        "    \n",
        "    dout_db2 =  1\n",
        "\n",
        "    grad_b2 = sum(dloss_dout*dout_db2)/64 #since bias is a vector, not a matrix, I use average to compute the gradient\n",
        "\n",
        "    dh2_dw2 = h_relu\n",
        "\n",
        "    grad_w2 = dh2_dw2.t().mm((dloss_dout * dout_dh2))\n",
        "\n",
        "    dh2_dhrelu = w2_2 \n",
        "\n",
        "    dh1b_db1 = 1 \n",
        "\n",
        "    dh1b_dh1 = 1 \n",
        "    \n",
        "    dh1_dw1 = x\n",
        "\n",
        "    grad_h_relu = (dloss_dout * dout_dh2).mm(dh2_dhrelu.t())\n",
        "    grad_h_relu[h_1 < 0] = 0\n",
        "\n",
        "    grad_b1 = sum(grad_h_relu * dh1b_db1)/64\n",
        "\n",
        "    grad_w1 = dh1_dw1.t().mm(grad_h_relu)\n",
        "    \n",
        "    w1_2 -= learning_rate * grad_w1\n",
        "    w2_2 -= learning_rate * grad_w2\n",
        "    b1_2 -= learning_rate * grad_b1\n",
        "    b2_2 -= learning_rate * grad_b2\n",
        "    \n",
        "    if t % 100 == 99:\n",
        "        print('Loss on iteration {} = {}'.format(t, loss))\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG5Gymlpk6x2"
      },
      "source": [
        "### PyTorch AutoGrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "67CBu6V9k6x2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106a6b7a-c025-42c2-9537-ff3bca51f66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 68.45321655273438\n",
            "199 67.99429321289062\n",
            "299 67.55693817138672\n",
            "399 67.14004516601562\n",
            "499 66.74253845214844\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(500):\n",
        "    y_pred = (x.mm(w1_3)+b1_3).clamp(min=0).mm(w2_3)+b2_3\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    loss.backward()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "        w1_3 -= learning_rate * w1_3.grad\n",
        "        w2_3 -= learning_rate * w2_3.grad\n",
        "        b1_3 -= learning_rate * b1_3.grad\n",
        "        b2_3 -= learning_rate * b2_3.grad        \n",
        "        \n",
        "        w1_3.grad.zero_()\n",
        "        w2_3.grad.zero_()\n",
        "        b1_3.grad.zero_()\n",
        "        b2_3.grad.zero_()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "sFVzZUgzk6x2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f4f7fc-a4d7-40f3-bb0c-f3af3156999e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 68.91714477539062\n",
            "199 68.90420532226562\n",
            "299 68.89129638671875\n",
            "399 68.8783950805664\n",
            "499 68.86551666259766\n"
          ]
        }
      ],
      "source": [
        "#automize learning with adaptive Adam\n",
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "optimizer = torch.optim.Adam([w1_4, w2_4, b1_4, b2_4], lr=learning_rate)\n",
        "\n",
        "for t in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    y_pred = (x.mm(w1_4)+b1_4).clamp(min=0).mm(w2_4)+b2_4\n",
        "    \n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    loss.backward()\n",
        "   \n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we see, that Autograd uses simple gradient desscent as default, since it is nearly the same as I have dome manulally and slightly different from the last case wher we used Adam. The trainig set is random, so the losses do not basically show anything: for another random dataset bias could woren performance and Adam would make it better"
      ],
      "metadata": {
        "id": "2O2Y_lfHjmhC"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "PyTorch.Graph.AutoGrad.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
